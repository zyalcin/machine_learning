{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3549412",
   "metadata": {},
   "source": [
    "# Total Gross Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc98820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and store feature vectors in array X and labels in array y\n",
    "import numpy as np\n",
    "DATAgross = np.loadtxt('imdb_adjusted_total_discrete.csv', delimiter=',', skiprows=1)  # Read data from csv file\n",
    "X = DATAgross[:, :-1]  # All columns except final column\n",
    "y = DATAgross[:, -1]   # Final column is label\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e23befc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 80% training and 20% testing. Set random_state=0.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe19939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression score on training data:\t-1224.684905845779\n",
      "Regression score on testing data:\t-878.4941184496165\n"
     ]
    }
   ],
   "source": [
    "# Train linear regression model on training data.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "Xtrainscaled = scaler.fit_transform(X_train)\n",
    "Xtestscaled = scaler.transform(X_test)\n",
    "\n",
    "regr = LinearRegression()  # Create LinearRegression instance\n",
    "regr.fit(X_train, y_train) # Learn hypothesis, i.e., find w_0 and w_1 for best fitting line\n",
    "print('Regression score on training data:\\t' + str(regr.score(Xtrainscaled, y_train)))\n",
    "print('Regression score on testing data:\\t' + str(regr.score(Xtestscaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8e76dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.32900835e-016 2.55477116e-011 7.54254675e-008 3.99657559e-004\n",
      " 6.26708171e-128 4.88978231e-001 4.19461129e-002 1.01164930e-040\n",
      " 0.00000000e+000]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "print(f_regression(X_train, y_train)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b31a1",
   "metadata": {},
   "source": [
    "Most significant: Director, followed by year, censor, and runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27baeb",
   "metadata": {},
   "source": [
    "# Predictors for Gross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d36e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron:\t0.9959166648932773\n",
      "Accuracy of RandomForest:\t1.0\n",
      "Accuracy of kNN:\t0.9970832712647102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic:\t0.9994169096209913\n",
      "Accuracy of SVM:\t0.9967917260752059\n",
      "Accuracy of NeuralNetwork:\t0.9967917260752059\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model  # Using sklearn Perceptron and Logistics classifier\n",
    "from sklearn import ensemble  # Using RandomForest classifier\n",
    "from sklearn import neighbors  # Using nearest neighbors classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "learners = {'Perceptron': linear_model.Perceptron(max_iter=10, random_state =0),\n",
    "            'RandomForest': ensemble.RandomForestClassifier(),\n",
    "            'kNN': neighbors.KNeighborsClassifier(metric = \"manhattan\"), \n",
    "            'logistic': linear_model.LogisticRegression(C = 10,random_state=0), #C is for regularization, which prevents overfitting with logistic regression\n",
    "            'SVM':SVC(C= 1000, gamma = 0.00001), \n",
    "            #C and gamma values vary based on the dataset. \n",
    "            #C controls the margin. larger Cs allow for less error. \n",
    "            #gamma controls how relevant the training data is. larger gamma values mean the training data has more weight, which can lead to overfitting.\n",
    "            #these values were chosen because they led to higher accuracy. \n",
    "            'NeuralNetwork': MLPClassifier()\n",
    "           }\n",
    "\n",
    "#for neural networks, kNN, and SVM\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "Xtrainscaled = scaler.fit_transform(X_train)\n",
    "Xtestscaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "for classifierName in learners:\n",
    "    #this has an error, but I think we need to do feature scaling for these?\n",
    "    if classifierName == 'NeuralNetwork' or classifierName =='kNN' or classifierName == 'SVM':\n",
    "        #print(Xtrainscaled.shape)\n",
    "        #print(y_train.shape)\n",
    "        learners[classifierName].fit(Xtrainscaled, y_train)\n",
    "        print('Accuracy of ' + classifierName + ':\\t' + str(np.mean(cross_val_score(learners[classifierName], Xtrainscaled, y_train))))\n",
    "    else:\n",
    "        learners[classifierName].fit(X_train, y_train)\n",
    "        print('Accuracy of ' + classifierName + ':\\t' + str(np.mean(cross_val_score(learners[classifierName], X_train, y_train))))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867d206",
   "metadata": {},
   "source": [
    "# Rating Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c333d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data and store feature vectors in array X and labels in array y\n",
    "import numpy as np\n",
    "\n",
    "DATAratings = np.loadtxt('imdb_rating_disrete.csv', delimiter=',', skiprows=1)  # Read data from csv file\n",
    "X = DATAratings[:, :-1]  # All columns except final column\n",
    "y = DATAratings[:, -1]   # Final column is label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef890835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 80% training and 20% testing. Set random_state=0.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51e6ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression score on test data:\t0.6246102430275409\n",
      "Regression score on train data:\t0.6244903495344294\n"
     ]
    }
   ],
   "source": [
    "# Train linear regression model on training data.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "Xtrainscaled = scaler.transform(X_train)\n",
    "Xtestscaled = scaler.transform(X_test)\n",
    "\n",
    "regr = LinearRegression()  # Create LinearRegression instance\n",
    "regr.fit(Xtrainscaled, y_train) # Learn hypothesis, i.e., find w_0 and w_1 for best fitting line\n",
    "print('Regression score on test data:\\t' + str(regr.score(Xtestscaled, y_test)))\n",
    "print('Regression score on train data:\\t' + str(regr.score(Xtrainscaled, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "291ba572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20725647e-24 4.46222511e-07 8.08470550e-76 7.40286550e-01\n",
      " 1.38238706e-01 3.16477985e-01 1.22318085e-06 8.58221903e-01\n",
      " 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "print(f_regression(X_train, y_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083c8b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:696: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Perceptron:\t0.6094005235045008\n",
      "Accuracy of RandomForest:\t1.0\n",
      "Accuracy of kNN:\t0.9329028962992914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic:\t0.8389376689153242\n",
      "Accuracy of SVM:\t0.9889136217573578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NeuralNetwork:\t0.9997084548104956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\macvi\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model  # Using sklearn Perceptron and Logistics classifier\n",
    "from sklearn import ensemble  # Using RandomForest classifier\n",
    "from sklearn import neighbors  # Using nearest neighbors classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "learners = {'Perceptron': linear_model.Perceptron(max_iter=10, random_state = 0),\n",
    "            'RandomForest': ensemble.RandomForestClassifier(),\n",
    "            'kNN': neighbors.KNeighborsClassifier(metric = \"manhattan\"), \n",
    "            'logistic': linear_model.LogisticRegression(C = 10, random_state=0),\n",
    "            'SVM':SVC(C= 1000, gamma = 0.00001),\n",
    "            'NeuralNetwork': MLPClassifier()\n",
    "           }\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "Xtrainscaled = scaler.transform(X_train)\n",
    "Xtestscaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "for classifierName in learners:\n",
    "    if classifierName == 'NeuralNetwork' or classifierName =='kNN' or classifierName == 'SVM':\n",
    "        learners[classifierName].fit(Xtrainscaled, y_train)\n",
    "        print('Accuracy of ' + classifierName + ':\\t' + str(np.mean(cross_val_score(learners[classifierName], Xtrainscaled, y_train))))\n",
    "    else:\n",
    "        learners[classifierName].fit(X_train, y_train)\n",
    "        print('Accuracy of ' + classifierName + ':\\t' + str(np.mean(cross_val_score(learners[classifierName], X_train, y_train))))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00877352-67d8-45a2-8bdb-f10930fd145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#does cross-val-score actually predict on testing data? We need to run the models on test data right? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
